\section{Introduction}
         \label{sec:intro}
\subsection{Preliminary Remarks}
            \label{sec:intro:rem}
Among the fundamental forces of nature the weak interactions clearly
show the most complicated and diversified pattern from the point of
view of our present day understanding represented by the
Standard Model of particle physics.
Although this theory of the strong and electroweak
forces is capable of describing very successfully a huge amount of
experimental information in a quantitative way
and a great deal of phenomena at least qualitatively, there are many
big question marks that remain. The most prominent among them like the
problem of electroweak symmetry breaking and the origin of
fermion masses and quark
mixing are closely related to the part of the Standard Model describing
weak interactions. Equally puzzling is the fact that whereas the
discrete space-time symmetries C, P, CP and T are respected by strong and
electromagnetic interactions, the weak force violates them all.
Obviously, the weak interaction is the corner of the Standard Model
that is least understood. The history of this field is full of
surprises and still more of them might be expected in the future.
\\
For these reasons big efforts have been and still are being
undertaken in order to develop our theoretical understanding of weak
interaction phenomena and to disentangle the basic mechanisms and
parameters.
An excellent laboratory for this enterprise is provided by the very
rich phenomenology of weak meson decays.

The careful investigation of these decays is mandatory for further
testing the Standard Model. Of particular importance is the
determination of all Cabibbo-Kobayashi-Maskawa (CKM) parameters as a
prerequisite for a decisive test of the consistency of the Standard
Model ansatz including the unitarity of the CKM matrix and its
compatibility with the quark masses.  Many interesting issues within
this context still remain unsettled. Let us just mention here the
question of direct CP violation in non-leptonic K decays
($\varepsilon'/\varepsilon$), the yet completely unknown pattern of CP
violation in the B system and the rare K and B decays, which are
sensitive to the effects of virtual heavy particles, most notably the
top quark, its mass and its weak couplings.  Whether the CKM
description of CP violation is correct, remains as an outstanding open
question.  It is clear that the need for a modification of the model is
conceivable and that meson decay phenomena might provide a window for
``new physics''. However, independently of this possibility it is
crucial to improve the theoretical predictions in the Standard Model
itself, either to further establish its correctness, or to be able to
make clear cut statements on its possible failure.

Now, for all attempts towards a theoretical understanding of these
issues the obvious fact that the fundamental forces do not come in
isolation is of crucial significance. Since hadrons are involved in the
decays that are of interest here, QCD unavoidably gets into the game.
In order to understand weak meson decays we have to understand the
interplay of weak interactions with the strong forces.\\
To accomplish this task it is necessary to employ the field
theoretical tools of the operator product expansion (OPE)
\cite{wilsonzim:72} and the renormalization group
\cite{stueckelberg:53}, \cite{gellmannlow:54}, \cite{ovsyannikov:56},
\cite{symanzik:70}, \cite{callan:70}, \cite{thooft:73},
\cite{weinberg:73}. The basic virtues of these two techniques may be
characterized as follows. Consider the amplitude $A$ for some weak
meson decay process. Using the OPE formalism this amplitude can be
represented as \cite{witten:77}
\begin{equation}
A=\langle {\cal H}_{eff}\rangle =\sum_i C_i(\mu, M_W)
                                        \langle Q_i(\mu)\rangle
\label{ahcq}
\end{equation}
where it is factorized into the Wilson coefficient functions $C_i$ and
the matrix elements of local operators $Q_i$. In this process the W
boson and other fields with mass bigger than the factorization scale
$\mu$
are ``integrated out'', that is removed from the theory as dynamical
degrees of freedom. The effect of their existence is however implicitly
taken into account in the Wilson coefficients. In a more intuitive
interpretation one can view the expression $\sum C_i Q_i$ as an effective
hamiltonian for the process considered, with $Q_i$ as the effective
vertices and $C_i$ the corresponding coupling constants. Usually for
weak decays only the operators of lowest dimension need to be taken into
account. Contributions of higher dimensional operators are negligible
since they are typically suppressed by powers of $p^2/M^2_W$, where $p$
is the momentum scale relevant for the decaying meson in question.
\\
The essential point about the OPE is that it achieves a separation of
the full problem into two distinct parts, the long-distance contributions
contained in the operator matrix elements and the short-distance physics
described by the Wilson coefficients. The renormalization scale $\mu$
separating the two regimes is typically chosen to be
of the order $\ord(\geq 1\gev)$ for
kaon decays and a few $GeV$ for the decays of D and B mesons.
The physical amplitude $A$ however cannot depend on
$\mu$. The $\mu$ dependence of the Wilson coefficients has
to cancel the $\mu$ dependence present in $\langle Q_i(\mu)\rangle$.
In other words it is a matter of choice what exactly
belongs to the matrix elements
and what to the coefficient functions. This cancellation of
$\mu$ dependence involves generally several terms in the expansion
in (\ref{ahcq}).\\
The long-distance part in (\ref{ahcq}) deals
with low energy strong interactions
and therefore poses a very difficult problem. Many approaches, like
lattice gauge theory, $1/N$- expansion, QCD- and hadronic sum rules or
chiral perturbation theory, have been used in the past to obtain
qualitative insight and some quantitative estimates of relevant
hadronic matrix elements. In addition heavy quark effective theory (HQET)
and heavy quark expansions (HQE) have been widely used for $B$ decays.
Despite these efforts the problem is not yet solved satisfactorily.
\\
In general in weak decays of mesons the hadronic matrix elements
constitute the most important source of theoretical uncertainty.  There
are however a few special examples of semileptonic rare decays ($\kpn$,
$K_L\to\pi^0\nu\bar\nu$, $B\to X_s\nu\bar\nu$) where the matrix
elements needed can be extracted from well measured leading decays or
calculated perturbatively or as in the case of $B_s \to \mu \bar \mu$
expressed fully in terms of meson decay constants. Thus practically the
problem of long-distance QCD can be completely avoided. This makes
these decay modes very attractive from a theoretical point of view,
although due to very small branching ratios they are quite difficult to
access experimentally today.
\\
Contrary to the long-distance contributions the short-distance part can
be analyzed systematically using well established field theoretical
methods. Due to the asymptotic freedom property of QCD the strong
interaction effects at short-distances are calculable in perturbation
theory in the strong coupling $\as(\mu)$. In fact $\as(\mu)$ is small
enough in the full range of relevant short distance scales of
$\ord(M_W)$ down to $\ord(1\gev)$ to serve as a reasonable expansion
parameter. However the presence of large logarithms $\ln(M_W/\mu)$
multiplying $\as(\mu)$ (where $\mu=\ord(1\gev)$) in the calculation of
the coefficients $C_i(\mu, M_W)$ spoils the validity of the usual
perturbation series. This is a characteristic feature of renormalizable
quantum field theories when vastly different scales are present.  It is
therefore necessary to perform a renormalization group analysis which
allows an efficient summation of logarithmic terms to all orders in
perturbation theory. In this way the usual perturbation theory is
replaced by the renormalization group improved perturbation theory in
which the leading order (LO) corresponds to summing the leading
logarithmic terms $\sim (\as \ln(M_W/\mu))^n$. Then at next-to-leading
order (NLO), all terms of the form $\sim\as (\as \ln(M_W/\mu))^n$ are
summed in addition, and so on.
\\
The evaluation of the short-distance coefficients in
renormalization group improved perturbation theory is only a part of
the entire problem, but one should stress that still it is
indispensible to analyze this part systematically; the effective
hamiltonians resulting from the short-distance analysis provide the
necessary basis for any further computation of weak decay amplitudes.
The long-distance matrix elements needed in addition
can be treated separately and will hopefully be known with
desirable accuracy one day.
\\
The rather formal expression for the decay amplitudes given in
\eqn{ahcq} can always be cast in a form \cite{buchallaetal:91}
\begin{equation}
A(M \to F)=\sum_i B_i \, V_{CKM}^{i} \, \eta^{i}_{QCD} \, F_i(m_t,m_c)
\label{PBEE}
\end{equation}
which is more useful for phenomenology.  In writing \eqn{PBEE} we
have generalized \eqn{ahcq} to include several CKM factors
$V_{CKM}^{i}$. The functions $F_i(m_t,m_c)$ result from the evaluation of
loop diagrams with internal top and charm exchanges  and may also depend
solely on $\mt$ or $\mc$. In certain cases $F_i$ are mass independent.
The factors $\eta^{i}_{QCD}$ summarize short distance QCD corrections
which can be calculated by the formal methods mentioned above. Finally
$B_i$ stand for nonperturbative factors related to the hadronic matrix
elements of the contributing operators: the main theoretical
uncertainty in the whole enterprise. A well known example of a
$B_i$-factor is the renormalization group invariant parameter $B_K$
relevant for $K^0-\bar K^0$ mixing and the indirect CP violation in
$K \to \pi\pi$.
\\
It is worth noting that the short-distance QCD contributions by
themselves have already an important impact on weak decay processes. In
non-leptonic K-decays, for example, they help to explain the famous
$\Delta I=1/2$ rule and they generate penguin operators which are
relevant for $\varepsilon' /\varepsilon$. They suppress the
semileptonic branching ratio in heavy quark decays and produce a
significant enhancement of the weak radiative process $B\to X_s\gamma$.

Starting with the pioneering work of \cite{gaillard:74} and
\cite{altarelli:74}, who calculated the first leading logarithmic QCD
effects in weak decays, considerable efforts have been devoted to the
calculation of short-distance QCD corrections to weak meson decay
processes. The analysis has been extended to a large variety of
particular modes. Of great interest are especially processes sensitive
to the virtual contribution of heavy quarks, like the top. A classic
example of this type is the 1974 analysis of \cite{gaillard:74b} of
$K^0 - \bar{K}^0$ mixing and their estimate of the charm quark mass
prior to its discovery, based on the dependence of the $\Delta S=2$
transition on virtual charm. This calculation constitutes the prototype
application for present day analyses of virtual top contributions in
$B^0 - \bar B^0$ mixing, rare decays and CP violation, which are
similar in spirit.
\\
Until 1989 most of the calculations were done in LO, i.e.\ in the
leading logarithmic approximation \cite{vainshtein:77},
\cite{gilman:79}, \cite{gilman:80}, \cite{guberina:80}. An exception
was the important work of \cite{altarelli:81} where the first NLO
calculation in the theory of weak decays has been presented.
\\
Today the effective hamiltonians for weak processes are available at
the next-to-leading level for the most important and interesting cases
due to a series of publications devoted to this enterprise beginning
with the work of \cite{burasweisz:90}. In table~\ref{tab:processes}
we give a list of decays for which NLO QCD corrections are known at
present.  With the next-to-leading short-distance effects included,
weak decays have in a sense now achieved the status that the
conceptually similar field of deep inelastic lepton nucleon scattering
had attained more then a decade ago \cite{buras:80}.

\begin{table}[htb]
\caption[]{Processes for which NLO QCD corrections have been calculated by
now.
\label{tab:processes}}
\begin{center}
\begin{tabular}{|cl|}
\bf Decay & \bf \phantom{XXXXXX} Reference \\
\hline
\hline
 \multicolumn{2}{|c|}{$\Delta F=1$ Decays} \\
\hline
current-current operators     & \cite{altarelli:81}, \cite{burasweisz:90} \\
QCD penguin operators         & \cite{burasetal:92b}, \cite{burasetal:92c}, \\
                              & \cite{ciuchini:93} \\
electroweak penguin operators & \cite{burasetal:92b}, \cite{burasetal:92c}, \\
                              & \cite{ciuchini:93} \\
magnetic penguin operators    & \cite{misiakmuenz:95} \\
$B(B \to X e\nu)$             & \cite{altarelli:81}, \cite{buchalla:93}, \\
                              & \cite{baganetal:94a}, \cite{baganetal:95} \\
Inclusive $\dS$               & \cite{jaminpich:94} \\
\hline
\multicolumn{2}{|c|}{Particle-Antiparticle Mixing} \\
\hline
$\eta_1$                   & \cite{herrlichnierste:93} \\
$\eta_2,~\eta_B$           & \cite{burasjaminweisz:90} \\
$\eta_3$                   & \cite{herrlichnierste:95} \\
\hline
\multicolumn{2}{|c|}{Rare K- and B-Meson Decays} \\
\hline
$K^0_L \rightarrow \pi^0\nu\bar{\nu}$, $B \rightarrow l^+l^-$,
$B \rightarrow X_{\rm s}\nu\bar{\nu}$ & \cite{buchallaburas:93b} \\
$K^+   \rightarrow \pi^+\nu\bar{\nu}$, $K_L \rightarrow \mu^+\mu^-$
                                      & \cite{buchallaburas:94} \\
$K^+\to\pi^+\mu\bar\mu$               & \cite{buchallaburas:94b} \\
$K_L \rightarrow \pi^0e^+e^-$         & \cite{burasetal:94a} \\
$B\rightarrow X_s e^+e^-$           & \cite{misiak:94}, \cite{burasmuenz:95} \\
\end{tabular}
\end{center}
\end{table}

Let us recall why NLO calculations are important for weak decays and why
it is worthwile to perform the very involved and complicated
computations.
\begin{itemize}
\item The NLO is first of all necessary to test the validity of
perturbation theory. In LO all the $(\as \ln(M_W/\mu))^n$ terms
are summed, yielding a result of $\ord(1)$; it is only at NLO
where one obtains a truly perturbative $\ord(\as)$
correction relative to the LO and one can check whether it is small
enough to justify the perturbative approach.
\item Without going to NLO the scheme specific
QCD scale $\Lambda_{\overline{MS}}$
extracted from various high energy processes cannot be used
meaningfully in weak decays.
\item Due to renormalization group (RG) invariance
the physical amplitudes do not depend on the exact scales $\mu_i$
at which quark masses (top) are defined or heavy particles are
integrated out. However in perturbation theory RG invariance is broken
through the truncation of the series by terms of the neglected order.
Numerically the resulting scale ambiguities, representing the
theoretical uncertainty of the short-distance part, are a serious
problem for the LO which can be reduced considerably by going to NLO.
\item The Wilson coefficients are renormalization
scheme dependent quantities. The
scheme dependence is first ``felt'' at NLO whereas the LO is completely
insensitive to this important feature. In particular this issue is
essential for a proper matching of the short distance contributions to
the long distance matrix elements as obtained from lattice calculations.
\item In some cases, particularly for $\epe$, $K_L\to\pi^0e^+e^-$ and
$B \to X_s e^+ e^-$, the central issue of the top quark mass dependence
is strictly speaking a NLO effect.
\end{itemize}
We would like to stress that short-distance QCD should be contrasted
with an ``intrinsically perturbative'' theory like QED, where
perturbation theory is almost the whole story since $\alpha_{QED}$ is
exceedingly small. In QCD the coupling is much larger at
interesting scales so that the conceptual questions like residual
scale or scheme dependences, which are formally of the neglected
higher order, become important numerically. Thus in this sense the
question of higher order corrections is not only one of a
quantitative improvement (of making precise predictions even more
accurate, like in QED), but of a qualitative improvement as well.

We think that the time is appropriate to review the subject of QCD
corrections to weak meson decays at the next-to-leading order level
and to collect the most important results obtained in this field.

\subsection{Outline}
            \label{sec:intro:outline}
This review is divided into three parts, roughly speaking ``basic
concepts'', ``technicalities'' and ``phenomenological applications''.
The division is made especially for pedagogical reasons hoping to make
the review as readable as possible to a wide audience of physicists.

In the first part we discuss the basic formalism necessary to obtain
the effective hamiltonians for weak decays from the underlying full
$SU(3) \otimes SU(2)_L \otimes U(1)_Y$ gauge theory of the Standard
Model.

The second part constitutes a compendium of effective hamiltonians for
all weak decays for which NLO corrections have been calculated in the
literature and whose list is given in table \ref{tab:processes}. We
include also the discussion of the important decay $B \to X_s \gamma$
which is known only at the LO level.

The third part of our review then presents the phenomenological picture
of weak decays beyond the leading logarithmic approximation using the
results obtained in parts one and two.

We end our review of this exciting field with a brief summary of
results and an outlook.

We are aware of the fact that some sections in this review are
necessarily rather technical which is connected to the very nature of
the subject of this review. We have however made efforts to present the
material in a pedagogical fashion. Thus part one can be regarded as an
elementary introduction to the formalism of QCD calculations which
include renormalization group methods  and the operator product
expansion. Even if our compendium in part two looks rather technical at
first sight, the guidelines to the effective hamiltonians presented in
section \ref{sec:Heffguide} should be helpful in following and using
this important part of our review. In any case the phenomenological part
three is almost self-contained and its material can be easily followed
with the help of the guidelines in section  \ref{sec:Heffguide} without
the necessity of fully understanding the details of NLO calculations.
